{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate ... with a fairness-constrained training task on the COMPAS dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part, we will compare two neural networks on the dataset - one with fairness constraints, one without - and compare the two.\n",
    "\n",
    "In the second part, we will use the same setup to compare two algorithms for constrained optimization: Augmented Lagrangian and Stochastic Ghost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu, sigmoid\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve, auc, log_loss\n",
    "\n",
    "SENS_ATTR_IND = 4\n",
    "SENSITIVE_CODE_0 = 0\n",
    "SENSITIVE_CODE_1 = 1\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "from humancompatible.train.tests.utils.datasets.load_compas import load_compas\n",
    "from humancompatible.train.optimize.auglagr import ALOptimizer\n",
    "from humancompatible.train.optimize.stochastic_ghost import StochasticGhost\n",
    "from humancompatible.train.connect.pytorch_connect import CustomNetwork, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Not saving again.\n",
      "File already exists. Not saving again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['race_code'] = df_needed['race'].map(race_mapping)\n",
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['crime_code'] = pd.Categorical(df_needed['c_charge_degree']).codes\n",
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['age_code'] = pd.Categorical(df_needed['age_cat']).codes\n",
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['race_code'] = df_needed['race'].map(race_mapping)\n",
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['gender_code'] = pd.Categorical(df_needed['sex']).codes\n",
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['score_code'] = pd.Categorical(df_needed['score_text']).codes\n",
      "c:\\Users\\andre\\phd\\humancompatible-train\\examples\\..\\humancompatible\\train\\tests\\utils\\datasets\\load_compas.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_needed['charge_degree_code'] = pd.Categorical(\n"
     ]
    }
   ],
   "source": [
    "x_train_raw, X_train, y_train, X_val, y_val = load_compas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_size = x_train_raw.shape[1]\n",
    "\n",
    "white_idx = (x_train_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_1)\n",
    "black_idx = (x_train_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0)\n",
    "x_black = X_train[black_idx, :]\n",
    "y_black = y_train[black_idx]\n",
    "x_white = X_train[white_idx, :]\n",
    "y_white = y_train[white_idx]\n",
    "\n",
    "x_train_raw = np.concatenate((x_train_raw[white_idx], x_train_raw[black_idx][:160]))\n",
    "    \n",
    "X_train = np.concatenate([x_white, x_black[:160]])\n",
    "y_train = np.concatenate([y_white, y_black[:160]])\n",
    "\n",
    "train_ds = TensorDataset(tensor(X_train[:, :ip_size], dtype=torch.float),tensor(y_train, dtype=torch.float))\n",
    "train_loader = DataLoader(train_ds, batch_size=16)\n",
    "    \n",
    "con_ds_b = TensorDataset(tensor(x_black[:160, :ip_size], dtype=torch.float),tensor(y_black[:160], dtype=torch.float))\n",
    "con_loader_b = DataLoader(con_ds_b, batch_size=16)\n",
    "con_ds_w = TensorDataset(tensor(x_white[:, :ip_size], dtype=torch.float),tensor(y_white, dtype=torch.float))\n",
    "con_loader_w = DataLoader(con_ds_w, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to demonstrate the effect of constraints, we make the dataset unbalanced with respect to the sensitive attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented Lagrangian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super().__init__()\n",
    "        self.lh = nn.Linear(n_in, n_hidden)\n",
    "        self.lo = nn.Linear(n_hidden, n_out)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden = relu(self.lh(input))\n",
    "        out = sigmoid(self.lo(hidden))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we use ```torch.utils.data.DataLoader``` to evaluate training loss batch by batch, we use ```DataLoader```'s to evaluate constraints. Here, we take two separate instances of the ```DataLoader``` class to sample equally sized batches of white and black individuals. Subject to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eq_loss_constr(loss, net, c_data, loss_bound):\n",
    "    w_inputs, w_labels = c_data[0]\n",
    "    b_inputs, b_labels = c_data[1]\n",
    "    w_outs = net(w_inputs)\n",
    "    w_loss = loss(w_outs, w_labels)\n",
    "    b_outs = net(b_inputs)\n",
    "    b_loss = loss(b_outs, b_labels)\n",
    "\n",
    "    return torch.abs(w_loss - b_loss) - loss_bound\n",
    "\n",
    "con_loader = [con_loader_w, con_loader_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = BCELoss()\n",
    "net = Net(7, 50, 1)\n",
    "loss_bound = 1e-2\n",
    "sampling_int = 1\n",
    "alo = ALOptimizer(net, loss, m_det = 0, m_st=1, st_constraint_fn=lambda net,\n",
    "                  d: eq_loss_constr(loss, net, d, loss_bound), lr = 5e-4, t=1.5)\n",
    "alo.optimize_(train_loader, con_loader, maxiter=30, epochs=5, constr_sampling_interval=sampling_int, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how well the trained model satisfies the constraints on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.0050, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cval = 0\n",
    "l = 0\n",
    "for d in zip(*[iter(c) for c in con_loader]):\n",
    "    cval += eq_loss_constr(BCELoss(), net, d, loss_bound)\n",
    "    l+=1\n",
    "print(l)\n",
    "cval/l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6605, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv = 0\n",
    "for input, label in zip(X_val, y_val):\n",
    "    lv += loss(net(torch.tensor(X_val, dtype=torch.float)), torch.tensor(y_val, dtype=torch.float))\n",
    "lv /= len(X_val)\n",
    "lv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Ghost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try doing the same with StochasticGhost. (LINK TO GHOST)\n",
    "\n",
    "**Note:** A unified interface is in development, so for now, Stochastic Ghost requires a bit more work to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operations:\n",
    "\n",
    "    # For now the input data is passed as init parameters\n",
    "    def __init__(self, data, net):\n",
    "\n",
    "        # Create a list of linear layers based on layer_sizes\n",
    "        self.x_train = data[0]\n",
    "        self.y_train = data[1]\n",
    "        self.x_val = data[2]\n",
    "        self.y_val = data[3]\n",
    "        self.x_train_raw = data[4]\n",
    "        self.model = net\n",
    "\n",
    "        black_idx = (self.x_train_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_1)\n",
    "        self.x_black = self.x_train[black_idx, :]\n",
    "        self.y_black = self.x_train[black_idx]\n",
    "\n",
    "        white_idx = (self.x_train_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0)\n",
    "        self.x_white = self.x_train[white_idx, :]\n",
    "        self.y_white = self.x_train[white_idx]\n",
    "\n",
    "    def obj_fun(self, params, minibatch):\n",
    "        x = self.x_train\n",
    "        y = self.y_train\n",
    "        model = self.model\n",
    "        samples = np.random.choice(len(y), minibatch, replace=False)\n",
    "        fval = model.get_obj(x[samples, :], y[samples], params)\n",
    "        return fval\n",
    "\n",
    "    def obj_grad(self, params, minibatch):\n",
    "        fgrad = []\n",
    "        x = self.x_train\n",
    "        y = self.y_train\n",
    "        model = self.model\n",
    "        samples = np.random.choice(len(y), minibatch, replace=False)\n",
    "        fgrad = model.get_obj_grad(x[samples, :], y[samples], params)\n",
    "        return fgrad\n",
    "\n",
    "    def conf1(self, params, minibatch):\n",
    "        model = self.model\n",
    "        \n",
    "        black_batch_idxs = np.random.choice(len(self.y_black), minibatch, replace=False)\n",
    "        white_batch_idxs = np.random.choice(len(self.y_white), minibatch, replace=False)\n",
    "   \n",
    "        conf1 = model.get_constraint(\n",
    "            self.x_black[black_batch_idxs, :], self.y_train[black_batch_idxs],\n",
    "            self.x_white[white_batch_idxs, :], self.y_train[white_batch_idxs],\n",
    "            params)\n",
    "        return conf1\n",
    "    \n",
    "    def conJ1(self, params, minibatch):\n",
    "        model = self.model\n",
    "\n",
    "        black_batch_idxs = np.random.choice(len(self.y_black), minibatch, replace=False)\n",
    "        white_batch_idxs = np.random.choice(len(self.y_white), minibatch, replace=False)\n",
    "\n",
    "        conj1 = model.get_constraint_grad(\n",
    "            self.x_black[black_batch_idxs, :], self.y_train[black_batch_idxs],\n",
    "            self.x_white[white_batch_idxs, :], self.y_train[white_batch_idxs],\n",
    "            params)\n",
    "        \n",
    "        return conj1\n",
    "\n",
    "    def conf2(self, params, minibatch):\n",
    "        model = self.model\n",
    "\n",
    "        black_batch_idxs = np.random.choice(len(self.y_black), minibatch, replace=False)\n",
    "        white_batch_idxs = np.random.choice(len(self.y_white), minibatch, replace=False)\n",
    "\n",
    "        conf2 = model.get_constraint(\n",
    "            self.x_black[black_batch_idxs, :], self.y_train[black_batch_idxs],\n",
    "            self.x_white[white_batch_idxs, :], self.y_train[white_batch_idxs],\n",
    "            params)\n",
    "        \n",
    "        return conf2\n",
    "    \n",
    "    def conJ2(self, params, minibatch):\n",
    "        model = self.model\n",
    "\n",
    "        black_batch_idxs = np.random.choice(len(self.y_black), minibatch, replace=False)\n",
    "        white_batch_idxs = np.random.choice(len(self.y_white), minibatch, replace=False)\n",
    "\n",
    "        conj2 = model.get_constraint_grad(\n",
    "            self.x_black[black_batch_idxs, :], self.y_train[black_batch_idxs],\n",
    "            self.x_white[white_batch_idxs, :], self.y_train[white_batch_idxs],\n",
    "            params)\n",
    "        \n",
    "        return conj2\n",
    "    \n",
    "def paramvals(maxiter, beta, rho, lamb, hess, tau, mbsz, numcon, geomp, stepdecay, gammazero, zeta, N, n, lossbound, scalef):\n",
    "    params = {\n",
    "        'maxiter': maxiter,  # number of iterations performed\n",
    "        'beta': beta,  # trust region size\n",
    "        'rho': rho,  # trust region for feasibility subproblem\n",
    "        'lamb': lamb,  # weight on the subfeasibility relaxation\n",
    "        'hess': hess,  # method of computing the Hessian of the QP, options include 'diag' 'lbfgs' 'fisher' 'adamdiag' 'adagraddiag'\n",
    "        'tau': tau,  # parameter for the hessian\n",
    "        'mbsz': mbsz,  # the standard minibatch size, used for evaluating the progress of the objective and constraint\n",
    "        'numcon': numcon,  # number of constraint functions\n",
    "        'geomp': geomp,  # parameter for the geometric random variable defining the number of subproblem samples\n",
    "        # strategy for step decrease, options include 'dimin' 'stepwise' 'slowdimin' 'constant'\n",
    "        'stepdecay': stepdecay,\n",
    "        'gammazero': gammazero,  # initial stepsize\n",
    "        'zeta': zeta,  # parameter associated with the stepsize iteration\n",
    "        'N': N,  # Train/val sample size\n",
    "        'n': n,  # Total number of parameters\n",
    "        'lossbound': lossbound, #Bound on constraint loss\n",
    "        'scalef': scalef #Scaling factor for constraints\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (X_train[:, :ip_size], y_train, X_val[:, :ip_size], y_val, x_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [ip_size, 50, 1]\n",
    "net = CustomNetwork((layer_sizes,))\n",
    "operations = Operations(data, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\miniconda3\\envs\\hcomp\\lib\\site-packages\\qpsolvers\\conversions\\ensure_sparse_matrices.py:24: UserWarning: Converted G to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build G as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\miniconda3\\envs\\hcomp\\lib\\site-packages\\qpsolvers\\conversions\\ensure_sparse_matrices.py:24: UserWarning: Converted A to scipy.sparse.csc.csc_matrix\n",
      "For best performance, build A as a scipy.sparse.csc_matrix rather than as a numpy.ndarray\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg iter time: 0.03790126256257185\n"
     ]
    }
   ],
   "source": [
    "initw, num_param = net.get_trainable_params()\n",
    "num_trials = min(len(y_train[((x_train_raw[:, SENS_ATTR_IND]) == SENSITIVE_CODE_1)]), len(y_train[(x_train_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0)]))\n",
    "params = paramvals(maxiter=1000, beta=10., rho=1e-3, lamb=0.5, hess='diag', tau=2., mbsz=100,\n",
    "                        numcon=2, geomp=0.2, stepdecay='dimin', gammazero=0.1, zeta=0.7, N=num_trials, n=num_param, lossbound=[loss_bound, loss_bound], scalef=[1., 1.])\n",
    "solver_params = {'max_iter': 400, 'eps_abs': 1e-9, 'polish': True, 'eps_prim_inf': 1e-6, 'eps_dual_inf': 1e-6}\n",
    "w, iterfs, itercs = StochasticGhost(operations.obj_fun, operations.obj_grad, [operations.conf1, operations.conf2], [operations.conJ1, operations.conJ2],\n",
    "                                    initw, params, solver_params=solver_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = net.state_dict()\n",
    "\n",
    "for k, val in zip(state_dict.keys(), w):\n",
    "    state_dict[k] = torch.tensor(val)\n",
    "\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how well the model satisfies the constraints on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0358, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cval = 0\n",
    "l = 0\n",
    "for d in zip(*[iter(c) for c in con_loader]):\n",
    "    cval += eq_loss_constr(BCELoss(), net, d, loss_bound)\n",
    "    l+=1\n",
    "print(l)\n",
    "cval/l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7179, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lv = 0\n",
    "for input, label in zip(X_val, y_val):\n",
    "    lv += loss(net(torch.tensor(X_val, dtype=torch.float)), torch.tensor(y_val, dtype=torch.float))\n",
    "lv /= len(X_val)\n",
    "lv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we use data from experiments conducted prior to compare the two algorithms on runtime, minimization and constraint satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'compas'\n",
    "MODEL = \"pytorch_connect\"\n",
    "MODEL_NAME = \"../humancompatible/train/connect/pytorch_connect\"\n",
    "DIRECTORY_PATH = \"../humancompatible/train/tests/utils/saved_models/\" + DATASET + '/' + MODEL\n",
    "FILE_EXT = '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = DIRECTORY_PATH\n",
    "\n",
    "# List all the files in the directory\n",
    "file_list = os.listdir(directory_path)\n",
    "\n",
    "# Filter files to select only model files (e.g., with '.pt' extension)\n",
    "model_files = [file for file in file_list if DATASET in file and file.endswith(FILE_EXT)]\n",
    "\n",
    "# Create an empty dictionary to store loaded models\n",
    "loaded_models = []\n",
    "\n",
    "# Load each model and store it in the dictionary\n",
    "for model_file in model_files:\n",
    "    # Extract model name from the file name\n",
    "    model_name = model_file.split('.')[0]\n",
    "    \n",
    "    # Load the model\n",
    "    model_load = load_model(directory_path, model_file)\n",
    "    \n",
    "    # Add the loaded model to the dictionary\n",
    "    loaded_models.append((model_file, model_load))\n",
    "\n",
    "\n",
    "df_raw = pd.read_csv(f'../humancompatible/train/tests/utils/datasets/raw_data/{DATASET}/val_data_raw_'+str(DATASET)+'.csv')\n",
    "df_scaled = pd.read_csv(f'../humancompatible/train/tests/utils/datasets/raw_data/{DATASET}/val_data_scaled_'+str(DATASET)+'.csv')\n",
    "x_raw = df_raw.values[:,:-1]\n",
    "x_scaled = df_scaled.values[:,:-1]\n",
    "y = df_scaled.values[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate some metrics for the models on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC_Sensitive_W</th>\n",
       "      <th>AUC_Sensitive_B</th>\n",
       "      <th>AUC_HM</th>\n",
       "      <th>Accuracy_W</th>\n",
       "      <th>Accuracy_B</th>\n",
       "      <th>Loss_W</th>\n",
       "      <th>Loss_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr0.pt</td>\n",
       "      <td>0.671265</td>\n",
       "      <td>0.710962</td>\n",
       "      <td>0.345272</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.589139</td>\n",
       "      <td>0.679073</td>\n",
       "      <td>0.679354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr1.pt</td>\n",
       "      <td>0.655470</td>\n",
       "      <td>0.693256</td>\n",
       "      <td>0.336917</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.671034</td>\n",
       "      <td>0.663727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr2.pt</td>\n",
       "      <td>0.682222</td>\n",
       "      <td>0.698908</td>\n",
       "      <td>0.345232</td>\n",
       "      <td>0.633224</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.675264</td>\n",
       "      <td>0.676811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr3.pt</td>\n",
       "      <td>0.684318</td>\n",
       "      <td>0.705214</td>\n",
       "      <td>0.347304</td>\n",
       "      <td>0.646382</td>\n",
       "      <td>0.633197</td>\n",
       "      <td>0.669348</td>\n",
       "      <td>0.669994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr4.pt</td>\n",
       "      <td>0.696780</td>\n",
       "      <td>0.675899</td>\n",
       "      <td>0.343090</td>\n",
       "      <td>0.659539</td>\n",
       "      <td>0.621926</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.675057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr0.pt</td>\n",
       "      <td>0.684986</td>\n",
       "      <td>0.720992</td>\n",
       "      <td>0.351264</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.659836</td>\n",
       "      <td>0.655715</td>\n",
       "      <td>0.649615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr1.pt</td>\n",
       "      <td>0.681798</td>\n",
       "      <td>0.714508</td>\n",
       "      <td>0.348885</td>\n",
       "      <td>0.669408</td>\n",
       "      <td>0.647541</td>\n",
       "      <td>0.658832</td>\n",
       "      <td>0.655177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr2.pt</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.714992</td>\n",
       "      <td>0.340733</td>\n",
       "      <td>0.639803</td>\n",
       "      <td>0.659836</td>\n",
       "      <td>0.651275</td>\n",
       "      <td>0.645164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr3.pt</td>\n",
       "      <td>0.667341</td>\n",
       "      <td>0.703168</td>\n",
       "      <td>0.342393</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.631148</td>\n",
       "      <td>0.650986</td>\n",
       "      <td>0.657380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr4.pt</td>\n",
       "      <td>0.640678</td>\n",
       "      <td>0.728294</td>\n",
       "      <td>0.340841</td>\n",
       "      <td>0.646382</td>\n",
       "      <td>0.651639</td>\n",
       "      <td>0.661562</td>\n",
       "      <td>0.654274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>net_al_compas_uncon.pt</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.613588</td>\n",
       "      <td>0.275501</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.683534</td>\n",
       "      <td>1.218050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr0.pt</td>\n",
       "      <td>0.564423</td>\n",
       "      <td>0.507613</td>\n",
       "      <td>0.267256</td>\n",
       "      <td>0.518092</td>\n",
       "      <td>0.529713</td>\n",
       "      <td>0.676985</td>\n",
       "      <td>0.698334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr1.pt</td>\n",
       "      <td>0.578590</td>\n",
       "      <td>0.628786</td>\n",
       "      <td>0.301322</td>\n",
       "      <td>0.458882</td>\n",
       "      <td>0.512295</td>\n",
       "      <td>0.699190</td>\n",
       "      <td>0.683696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr2.pt</td>\n",
       "      <td>0.489706</td>\n",
       "      <td>0.654223</td>\n",
       "      <td>0.280067</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.531762</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.686192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr3.pt</td>\n",
       "      <td>0.637379</td>\n",
       "      <td>0.667303</td>\n",
       "      <td>0.325999</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.512295</td>\n",
       "      <td>0.707541</td>\n",
       "      <td>0.681947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr4.pt</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>0.461370</td>\n",
       "      <td>0.224745</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.696595</td>\n",
       "      <td>0.700287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  AUC_Sensitive_W  AUC_Sensitive_B  \\\n",
       "0     net_al_compas_lb0.01_s1_tr0.pt         0.671265         0.710962   \n",
       "1     net_al_compas_lb0.01_s1_tr1.pt         0.655470         0.693256   \n",
       "2     net_al_compas_lb0.01_s1_tr2.pt         0.682222         0.698908   \n",
       "3     net_al_compas_lb0.01_s1_tr3.pt         0.684318         0.705214   \n",
       "4     net_al_compas_lb0.01_s1_tr4.pt         0.696780         0.675899   \n",
       "5   net_al_compas_lb0.01_sexp_tr0.pt         0.684986         0.720992   \n",
       "6   net_al_compas_lb0.01_sexp_tr1.pt         0.681798         0.714508   \n",
       "7   net_al_compas_lb0.01_sexp_tr2.pt         0.650944         0.714992   \n",
       "8   net_al_compas_lb0.01_sexp_tr3.pt         0.667341         0.703168   \n",
       "9   net_al_compas_lb0.01_sexp_tr4.pt         0.640678         0.728294   \n",
       "10            net_al_compas_uncon.pt         0.500000         0.613588   \n",
       "11    net_ghost_compas_lb0.01_tr0.pt         0.564423         0.507613   \n",
       "12    net_ghost_compas_lb0.01_tr1.pt         0.578590         0.628786   \n",
       "13    net_ghost_compas_lb0.01_tr2.pt         0.489706         0.654223   \n",
       "14    net_ghost_compas_lb0.01_tr3.pt         0.637379         0.667303   \n",
       "15    net_ghost_compas_lb0.01_tr4.pt         0.438208         0.461370   \n",
       "\n",
       "      AUC_HM  Accuracy_W  Accuracy_B    Loss_W    Loss_B  \n",
       "0   0.345272    0.651316    0.589139  0.679073  0.679354  \n",
       "1   0.336917    0.625000    0.625000  0.671034  0.663727  \n",
       "2   0.345232    0.633224    0.626025  0.675264  0.676811  \n",
       "3   0.347304    0.646382    0.633197  0.669348  0.669994  \n",
       "4   0.343090    0.659539    0.621926  0.672200  0.675057  \n",
       "5   0.351264    0.684211    0.659836  0.655715  0.649615  \n",
       "6   0.348885    0.669408    0.647541  0.658832  0.655177  \n",
       "7   0.340733    0.639803    0.659836  0.651275  0.645164  \n",
       "8   0.342393    0.656250    0.631148  0.650986  0.657380  \n",
       "9   0.340841    0.646382    0.651639  0.661562  0.654274  \n",
       "10  0.275501    0.585526    0.606557  0.683534  1.218050  \n",
       "11  0.267256    0.518092    0.529713  0.676985  0.698334  \n",
       "12  0.301322    0.458882    0.512295  0.699190  0.683696  \n",
       "13  0.280067    0.414474    0.531762  0.703758  0.686192  \n",
       "14  0.325999    0.414474    0.512295  0.707541  0.681947  \n",
       "15  0.224745    0.457237    0.469262  0.696595  0.700287  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for model_index, model_iter in enumerate(loaded_models):\n",
    "    # Set the model to evaluation mode\n",
    "    (model_name, model) = model_iter\n",
    "    model.eval()\n",
    "\n",
    "    predictions_0 = model.evaluate(model.to_backend(x_scaled[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0), :]))\n",
    "    predictions_1 = model.evaluate(model.to_backend(x_scaled[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_1), :]))\n",
    "\n",
    "    acc_0 = np.sum(y[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0)] == (predictions_0[:,0] > 0.5)) / len(predictions_0)\n",
    "    acc_1 = np.sum(y[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_1)] == (predictions_1[:,0] > 0.5)) / len(predictions_1)\n",
    "    l_0 = log_loss(y[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0)], predictions_0[:,0])\n",
    "    l_1 = log_loss(y[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_1)], predictions_1[:,0])\n",
    "    \n",
    "    fpr_0, tpr_0, thresholds_0 = roc_curve(y[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_0)], predictions_0)\n",
    "    auc_0 = auc(fpr_0, tpr_0)\n",
    "\n",
    "    fpr_1, tpr_1, thresholds_1 = roc_curve(y[(x_raw[:, SENS_ATTR_IND] == SENSITIVE_CODE_1)], predictions_1)\n",
    "    auc_1 = auc(fpr_1, tpr_1)\n",
    "\n",
    "    auc_hm = (auc_0*auc_1)/(auc_0 + auc_1)\n",
    "    \n",
    "    tpr_minus_fpr_0 = tpr_0 - fpr_0\n",
    "    optimal_threshold_index_0 = np.argmax(tpr_minus_fpr_0)\n",
    "    optimal_threshold_0 = thresholds_0[optimal_threshold_index_0]\n",
    "\n",
    "    tpr_minus_fpr_1 = tpr_1 - fpr_1\n",
    "    optimal_threshold_index_1 = np.argmax(tpr_minus_fpr_1)\n",
    "    optimal_threshold_1 = thresholds_1[optimal_threshold_index_1]\n",
    "\n",
    "    results_list.append({'Model': str(model_name),\n",
    "                                    'AUC_Sensitive_W': auc_0,\n",
    "                                    'AUC_Sensitive_B': auc_1,\n",
    "                                    'AUC_HM' : auc_hm,\n",
    "                                    'Accuracy_W': acc_0,\n",
    "                                    'Accuracy_B': acc_1,\n",
    "                                    'Loss_W': l_0,\n",
    "                                    'Loss_B': l_1,\n",
    "                                    # 'Optimal_Threshold_0': optimal_threshold_0,\n",
    "                                    # 'Optimal_Threshold_1': optimal_threshold_1\n",
    "                                    })\n",
    "    \n",
    "res_df = pd.DataFrame(results_list)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort by descending constraint violation (absolute difference between loss value on subgroups on validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AUC_Sensitive_W</th>\n",
       "      <th>AUC_Sensitive_B</th>\n",
       "      <th>AUC_HM</th>\n",
       "      <th>Accuracy_W</th>\n",
       "      <th>Accuracy_B</th>\n",
       "      <th>Loss_W</th>\n",
       "      <th>Loss_B</th>\n",
       "      <th>loss_abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr0.pt</td>\n",
       "      <td>0.671265</td>\n",
       "      <td>0.710962</td>\n",
       "      <td>0.345272</td>\n",
       "      <td>0.651316</td>\n",
       "      <td>0.589139</td>\n",
       "      <td>0.679073</td>\n",
       "      <td>0.679354</td>\n",
       "      <td>0.000281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr3.pt</td>\n",
       "      <td>0.684318</td>\n",
       "      <td>0.705214</td>\n",
       "      <td>0.347304</td>\n",
       "      <td>0.646382</td>\n",
       "      <td>0.633197</td>\n",
       "      <td>0.669348</td>\n",
       "      <td>0.669994</td>\n",
       "      <td>0.000645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr2.pt</td>\n",
       "      <td>0.682222</td>\n",
       "      <td>0.698908</td>\n",
       "      <td>0.345232</td>\n",
       "      <td>0.633224</td>\n",
       "      <td>0.626025</td>\n",
       "      <td>0.675264</td>\n",
       "      <td>0.676811</td>\n",
       "      <td>0.001547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr4.pt</td>\n",
       "      <td>0.696780</td>\n",
       "      <td>0.675899</td>\n",
       "      <td>0.343090</td>\n",
       "      <td>0.659539</td>\n",
       "      <td>0.621926</td>\n",
       "      <td>0.672200</td>\n",
       "      <td>0.675057</td>\n",
       "      <td>0.002858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr1.pt</td>\n",
       "      <td>0.681798</td>\n",
       "      <td>0.714508</td>\n",
       "      <td>0.348885</td>\n",
       "      <td>0.669408</td>\n",
       "      <td>0.647541</td>\n",
       "      <td>0.658832</td>\n",
       "      <td>0.655177</td>\n",
       "      <td>0.003655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr4.pt</td>\n",
       "      <td>0.438208</td>\n",
       "      <td>0.461370</td>\n",
       "      <td>0.224745</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.469262</td>\n",
       "      <td>0.696595</td>\n",
       "      <td>0.700287</td>\n",
       "      <td>0.003692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr0.pt</td>\n",
       "      <td>0.684986</td>\n",
       "      <td>0.720992</td>\n",
       "      <td>0.351264</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.659836</td>\n",
       "      <td>0.655715</td>\n",
       "      <td>0.649615</td>\n",
       "      <td>0.006100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr2.pt</td>\n",
       "      <td>0.650944</td>\n",
       "      <td>0.714992</td>\n",
       "      <td>0.340733</td>\n",
       "      <td>0.639803</td>\n",
       "      <td>0.659836</td>\n",
       "      <td>0.651275</td>\n",
       "      <td>0.645164</td>\n",
       "      <td>0.006111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr3.pt</td>\n",
       "      <td>0.667341</td>\n",
       "      <td>0.703168</td>\n",
       "      <td>0.342393</td>\n",
       "      <td>0.656250</td>\n",
       "      <td>0.631148</td>\n",
       "      <td>0.650986</td>\n",
       "      <td>0.657380</td>\n",
       "      <td>0.006395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>net_al_compas_lb0.01_sexp_tr4.pt</td>\n",
       "      <td>0.640678</td>\n",
       "      <td>0.728294</td>\n",
       "      <td>0.340841</td>\n",
       "      <td>0.646382</td>\n",
       "      <td>0.651639</td>\n",
       "      <td>0.661562</td>\n",
       "      <td>0.654274</td>\n",
       "      <td>0.007288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>net_al_compas_lb0.01_s1_tr1.pt</td>\n",
       "      <td>0.655470</td>\n",
       "      <td>0.693256</td>\n",
       "      <td>0.336917</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.671034</td>\n",
       "      <td>0.663727</td>\n",
       "      <td>0.007307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr1.pt</td>\n",
       "      <td>0.578590</td>\n",
       "      <td>0.628786</td>\n",
       "      <td>0.301322</td>\n",
       "      <td>0.458882</td>\n",
       "      <td>0.512295</td>\n",
       "      <td>0.699190</td>\n",
       "      <td>0.683696</td>\n",
       "      <td>0.015494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr2.pt</td>\n",
       "      <td>0.489706</td>\n",
       "      <td>0.654223</td>\n",
       "      <td>0.280067</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.531762</td>\n",
       "      <td>0.703758</td>\n",
       "      <td>0.686192</td>\n",
       "      <td>0.017566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr0.pt</td>\n",
       "      <td>0.564423</td>\n",
       "      <td>0.507613</td>\n",
       "      <td>0.267256</td>\n",
       "      <td>0.518092</td>\n",
       "      <td>0.529713</td>\n",
       "      <td>0.676985</td>\n",
       "      <td>0.698334</td>\n",
       "      <td>0.021349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>net_ghost_compas_lb0.01_tr3.pt</td>\n",
       "      <td>0.637379</td>\n",
       "      <td>0.667303</td>\n",
       "      <td>0.325999</td>\n",
       "      <td>0.414474</td>\n",
       "      <td>0.512295</td>\n",
       "      <td>0.707541</td>\n",
       "      <td>0.681947</td>\n",
       "      <td>0.025594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>net_al_compas_uncon.pt</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.613588</td>\n",
       "      <td>0.275501</td>\n",
       "      <td>0.585526</td>\n",
       "      <td>0.606557</td>\n",
       "      <td>0.683534</td>\n",
       "      <td>1.218050</td>\n",
       "      <td>0.534515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Model  AUC_Sensitive_W  AUC_Sensitive_B  \\\n",
       "0     net_al_compas_lb0.01_s1_tr0.pt         0.671265         0.710962   \n",
       "3     net_al_compas_lb0.01_s1_tr3.pt         0.684318         0.705214   \n",
       "2     net_al_compas_lb0.01_s1_tr2.pt         0.682222         0.698908   \n",
       "4     net_al_compas_lb0.01_s1_tr4.pt         0.696780         0.675899   \n",
       "6   net_al_compas_lb0.01_sexp_tr1.pt         0.681798         0.714508   \n",
       "15    net_ghost_compas_lb0.01_tr4.pt         0.438208         0.461370   \n",
       "5   net_al_compas_lb0.01_sexp_tr0.pt         0.684986         0.720992   \n",
       "7   net_al_compas_lb0.01_sexp_tr2.pt         0.650944         0.714992   \n",
       "8   net_al_compas_lb0.01_sexp_tr3.pt         0.667341         0.703168   \n",
       "9   net_al_compas_lb0.01_sexp_tr4.pt         0.640678         0.728294   \n",
       "1     net_al_compas_lb0.01_s1_tr1.pt         0.655470         0.693256   \n",
       "12    net_ghost_compas_lb0.01_tr1.pt         0.578590         0.628786   \n",
       "13    net_ghost_compas_lb0.01_tr2.pt         0.489706         0.654223   \n",
       "11    net_ghost_compas_lb0.01_tr0.pt         0.564423         0.507613   \n",
       "14    net_ghost_compas_lb0.01_tr3.pt         0.637379         0.667303   \n",
       "10            net_al_compas_uncon.pt         0.500000         0.613588   \n",
       "\n",
       "      AUC_HM  Accuracy_W  Accuracy_B    Loss_W    Loss_B  loss_abs_diff  \n",
       "0   0.345272    0.651316    0.589139  0.679073  0.679354       0.000281  \n",
       "3   0.347304    0.646382    0.633197  0.669348  0.669994       0.000645  \n",
       "2   0.345232    0.633224    0.626025  0.675264  0.676811       0.001547  \n",
       "4   0.343090    0.659539    0.621926  0.672200  0.675057       0.002858  \n",
       "6   0.348885    0.669408    0.647541  0.658832  0.655177       0.003655  \n",
       "15  0.224745    0.457237    0.469262  0.696595  0.700287       0.003692  \n",
       "5   0.351264    0.684211    0.659836  0.655715  0.649615       0.006100  \n",
       "7   0.340733    0.639803    0.659836  0.651275  0.645164       0.006111  \n",
       "8   0.342393    0.656250    0.631148  0.650986  0.657380       0.006395  \n",
       "9   0.340841    0.646382    0.651639  0.661562  0.654274       0.007288  \n",
       "1   0.336917    0.625000    0.625000  0.671034  0.663727       0.007307  \n",
       "12  0.301322    0.458882    0.512295  0.699190  0.683696       0.015494  \n",
       "13  0.280067    0.414474    0.531762  0.703758  0.686192       0.017566  \n",
       "11  0.267256    0.518092    0.529713  0.676985  0.698334       0.021349  \n",
       "14  0.325999    0.414474    0.512295  0.707541  0.681947       0.025594  \n",
       "10  0.275501    0.585526    0.606557  0.683534  1.218050       0.534515  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df['loss_abs_diff'] = abs(res_df.Loss_W - res_df.Loss_B)\n",
    "res_df.sort_values(by='loss_abs_diff', ascending=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
