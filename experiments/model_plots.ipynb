{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "from utils.load_folktables import load_folktables_torch\n",
    "from src.algorithms.c_utils.constraint_fns import *\n",
    "from fairret.statistic import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents some useful plots based on the performance of the trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Preparation**\n",
    "\n",
    "**Load the Folktables dataset for the selected state and prepare it for usage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK = 'income'\n",
    "# TASK = 'employment'\n",
    "STATE = 'OK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, [w_idx_train, nw_idx_train], X_test, y_test, [w_idx_test, nw_idx_test] = load_folktables_torch(\n",
    "        TASK, state=STATE, random_state=42, make_unbalanced = False, onehot=False\n",
    "    )\n",
    "\n",
    "sensitive_value_0 = 'white'\n",
    "sensitive_value_1 = 'non-white'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() and False else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = tensor(y_train, dtype=torch.float, device=device)\n",
    "    \n",
    "X_test_tensor = tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = tensor(y_test, dtype=torch.float, device=device)\n",
    "    \n",
    "X_train_w = X_train_tensor[w_idx_train]\n",
    "y_train_w = y_train_tensor[w_idx_train]\n",
    "X_train_nw = X_train_tensor[nw_idx_train]\n",
    "y_train_nw = y_train_tensor[nw_idx_train]\n",
    "    \n",
    "X_test_w = X_test_tensor[w_idx_test]\n",
    "y_test_w = y_test_tensor[w_idx_test]\n",
    "X_test_nw = X_test_tensor[nw_idx_test]\n",
    "y_test_nw = y_test_tensor[nw_idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w, nw, total')\n",
    "print('train')\n",
    "print(len(y_train_w), len(y_train_nw), len(y_train))\n",
    "print(sum(y_train_w == 1)/len(y_train_w), sum(y_train_nw == 1)/len(y_train_nw), sum(y_train_tensor == 1)/len(y_train_tensor))\n",
    "print('test')\n",
    "print(len(y_test_w), len(y_test_nw), len(y_test))\n",
    "print(sum(y_test_w == 1)/len(y_test_w), sum(y_test_nw == 1)/len(y_test_nw), sum(y_test_tensor == 1)/len(y_test_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_BOUND = 0.005\n",
    "DATASET = TASK + '_' + STATE\n",
    "constraint = 'loss'\n",
    "DIRECTORY_PATH = \"./utils/saved_models/\" + DATASET + '/'+ constraint + '/' + f'{LOSS_BOUND:.0e}' + '/'\n",
    "FILE_EXT = '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_shape, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, out_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = DIRECTORY_PATH\n",
    "file_list = os.listdir(directory_path)\n",
    "model_files = [file for file in file_list if file.endswith(FILE_EXT)]\n",
    "for model_file in model_files:\n",
    "    # if model_file.startswith('sg_a'):\n",
    "    #     continue\n",
    "    model_name = model_file\n",
    "    model = SimpleNet(X_test.shape[1], 1).to(device)\n",
    "    print(model_file, end='\\r')\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(directory_path + model_name, weights_only=False, map_location=device))\n",
    "    except:\n",
    "        continue\n",
    "    model_file = str.join('', model_file.split('_trial')[:-1])\n",
    "    loaded_models.append((model_file, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alg_name(alg:str):\n",
    "    if alg.startswith('swsg'):\n",
    "        return 'Switching Subgradient'\n",
    "    elif alg.startswith('sgd'):\n",
    "        return 'SGD'\n",
    "    elif alg.startswith('sg'):\n",
    "        return 'Stochastic Ghost'\n",
    "    elif alg.startswith('sslalm_mu0'):\n",
    "        return 'ALM'\n",
    "    elif alg.startswith('sslalm'):\n",
    "        return 'SSL-ALM'\n",
    "    elif alg.startswith('fairret'):\n",
    "        return 'SGD + Fairret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average(stats, count):\n",
    "    stats.total_calls /= count\n",
    "    stats.prim_calls /= count\n",
    "    stats.total_tt /= count\n",
    "\n",
    "    for func, source in stats.stats.items():\n",
    "        cc, nc, tt, ct, callers = source\n",
    "        stats.stats[func] = ( cc/count, nc/count, tt/count, ct/count, callers )\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define some fairness metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairret.statistic import *\n",
    "\n",
    "\n",
    "def fair_stats(p_1, y_1, p_2, y_2):\n",
    "    p = torch.concat([torch.tensor(p_1), torch.tensor(p_2)]).unsqueeze(1)\n",
    "    w_onehot = torch.tensor([[0., 1.]]*len(p_1))\n",
    "    b_onehot = torch.tensor([[1., 0.]]*len(p_2))\n",
    "    sens = torch.vstack([w_onehot,b_onehot])\n",
    "    labels = torch.concat([torch.tensor(y_1), torch.tensor(y_2)]).unsqueeze(1)\n",
    "    pr0, pr1 = PositiveRate()(p, sens)\n",
    "    fpr0, fpr1 = FalsePositiveRate()(p, sens, labels)\n",
    "    tpr0, tpr1 = TruePositiveRate()(p, sens, labels)\n",
    "    tnr0, tnr1 = 1-fpr0, 1-fpr1\n",
    "    fnr0, fnr1 = 1-tpr0, 1-tpr1\n",
    "    acc0, acc1 = Accuracy()(p, sens, labels)\n",
    "    ppv0, ppv1 = PositivePredictiveValue()(p, sens, labels)\n",
    "    fomr0, fomr1 = FalseOmissionRate()(p,sens,labels)\n",
    "    npv0, npv1 = 1- fomr0, 1-fomr1 \n",
    "    \n",
    "    ind = abs(pr0 - pr1)\n",
    "    sp = abs(tpr0 - tpr1) + abs(fpr0 - fpr1)\n",
    "    \n",
    "    ina = sum(np.concatenate([p_1, p_2]) != np.concatenate([y_1, y_2])) / (len(p_1) + len(p_2))\n",
    "    # ina = (1-np.mean([acc0, acc1]))\n",
    "    sf = abs(ppv0 - ppv1) + abs(npv0 - npv1)\n",
    "    return ind, sp, ina, sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate test set statistics for the models - AUC, constraint satisfaction, loss, etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ot\n",
    "\n",
    "@torch.inference_mode()\n",
    "def make_model_stats_table(X_w, y_w, X_nw, y_nw):\n",
    "    \n",
    "    results_list = []\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for model_index, model_iter in enumerate(loaded_models):\n",
    "        (model_name, model) = model_iter\n",
    "        \n",
    "        # else:\n",
    "        alg = model_name\n",
    "        predictions_0 = model(X_w)\n",
    "        predictions_1 = model(X_nw)\n",
    "        if torch.any(torch.isnan(predictions_0)) or torch.any(torch.isnan(predictions_1)):\n",
    "            print(f'skipped {model_name}')\n",
    "            continue\n",
    "        y_w = y_w.squeeze()\n",
    "        y_nw = y_nw.squeeze()\n",
    "        l_0 = loss_fn(predictions_0[:,0], y_w).cpu().numpy()\n",
    "        l_1 = loss_fn(predictions_1[:,0], y_nw).cpu().numpy()\n",
    "        predictions_0 = torch.nn.functional.sigmoid(predictions_0[:,0])\n",
    "        predictions_1 = torch.nn.functional.sigmoid(predictions_1[:,0])\n",
    "        # Calculate AUCs for sensitive attribute 0\n",
    "        fpr_0, tpr_0, thresholds_0 = roc_curve(y_w.cpu().numpy(), predictions_0.cpu().numpy())\n",
    "        auc_0 = auc(fpr_0, tpr_0)\n",
    "        # Calculate AUCs for sensitive attribute 1\n",
    "        fpr_1, tpr_1, thresholds_1 = roc_curve(y_nw.cpu().numpy(), predictions_1.cpu().numpy())\n",
    "        auc_1 = auc(fpr_1, tpr_1)\n",
    "        auc_hm = (auc_0*auc_1)/(auc_0 + auc_1)\n",
    "        auc_m = (auc_0+auc_1)/2\n",
    "        # Calculate TPR-FPR difference for sensitive attribute 0\n",
    "        tpr_minus_fpr_0 = tpr_0 - fpr_0\n",
    "        optimal_threshold_index_0 = np.argmax(tpr_minus_fpr_0)\n",
    "        optimal_threshold_0 = thresholds_0[optimal_threshold_index_0]\n",
    "\n",
    "        # Calculate TPR-FPR difference for sensitive attribute 1\n",
    "        tpr_minus_fpr_1 = tpr_1 - fpr_1\n",
    "        optimal_threshold_index_1 = np.argmax(tpr_minus_fpr_1)\n",
    "        optimal_threshold_1 = thresholds_1[optimal_threshold_index_1]\n",
    "        \n",
    "        p_0_np = (predictions_0 > 0.5).cpu().numpy()\n",
    "        p_1_np = (predictions_1 > 0.5).cpu().numpy()\n",
    "        y_w_np = y_w.cpu().numpy()\n",
    "        y_nw_np = y_nw.cpu().numpy()\n",
    "        \n",
    "        ind, sp, ina, sf = fair_stats(p_0_np, y_w_np, p_1_np, y_nw_np)\n",
    "\n",
    "        a0, x0 = np.histogram(predictions_0, bins=50)\n",
    "        a1, x1 = np.histogram(predictions_1, bins=x0)\n",
    "        a0 = a0.astype(float)\n",
    "        a1 = a1.astype(float)\n",
    "        a0 /= np.sum(a0)\n",
    "        a1 /= np.sum(a1)\n",
    "        wd = ot.wasserstein_1d(x0[1:], x1[1:], a0, a1, p=2)\n",
    "        # Store results in the DataFrame\n",
    "        results_list.append({'Model': str(model_name),\n",
    "                             'Algorithm': alg,\n",
    "                                        'AUC_M' : auc_m,\n",
    "                                        'Ind': ind,\n",
    "                                        'Sp': sp,\n",
    "                                        'Ina': ina,\n",
    "                                        'Sf': sf,\n",
    "                                        'Wd': wd,\n",
    "                                        '|Loss_0 - Loss_1|': abs(l_0 - l_1)\n",
    "                                        })\n",
    "        \n",
    "    res_df = pd.DataFrame(results_list)\n",
    "    return res_df\n",
    "\n",
    "\n",
    "def aggregate_model_stats_table(table: pd.DataFrame, agg_fns):\n",
    "    if len(agg_fns) == 1 and not isinstance(agg_fns, str):\n",
    "        df = table.drop('Model', axis=1).groupby('Algorithm').agg(agg_fns[0]).sort_index()\n",
    "    else:\n",
    "        df = table.drop('Model', axis=1).groupby('Algorithm').agg(agg_fns)\n",
    "\n",
    "    df['Algname'] = df.apply(lambda row: get_alg_name(row.name), axis=1)\n",
    "    df['Algname'] = pd.Categorical(df['Algname'], ['SGD', 'SGD + Fairret', 'Stochastic Ghost', 'ALM', 'SSL-ALM', 'Switching Subgradient'])\n",
    "    df = df.sort_values(by='Algname', axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregate statistics by algorithm:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_train= make_model_stats_table(X_train_w, y_train_w, X_train_nw, y_train_nw)\n",
    "\n",
    "train_df = aggregate_model_stats_table(res_df_train, 'mean')\n",
    "train_df_std = aggregate_model_stats_table(res_df_train, ['mean', 'std'])\n",
    "train_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test set**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_test= make_model_stats_table(X_test_w, y_test_w, X_test_nw, y_test_nw)\n",
    "\n",
    "test_df = aggregate_model_stats_table(res_df_test, 'mean')\n",
    "test_df_std = aggregate_model_stats_table(res_df_test, ['mean', 'std'])\n",
    "test_df_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in test_df.index:\n",
    "    alg_name = 'sslalm_aug' if model_name.startswith('sslalm_mu0') else model_name.split('_')[0]\n",
    "    os.makedirs(os.path.dirname(f'./plots/{alg_name}/{DATASET}/'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spider_line(data, title=None):\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "    labels = ['Ind', 'Sep', 'Ina', 'Suf']\n",
    "    # Number of variables we're plotting.\n",
    "    num_vars = len(labels)\n",
    "\n",
    "    # Split the circle into even parts and save the angles\n",
    "    # so we know where to put each axis.\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "    # The plot is a circle, so we need to \"complete the loop\"\n",
    "    # and append the start value to the end.\n",
    "    angles += angles[:1]\n",
    "    labels += labels[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,10), subplot_kw=dict(polar=True))\n",
    "\n",
    "    for alg in data.index:\n",
    "        values = data.loc[alg, ['Ind', 'Sp', 'Ina','Sf', 'Ind']].tolist()\n",
    "        ax.plot(angles, values, lw=2, label=get_alg_name(alg))\n",
    "        # ax.plot(angles, values, lw=2, label=alg)\n",
    "        ax.set_yticks([0,0.1,0.2, 0.3])\n",
    "\n",
    "    plt.thetagrids(np.degrees(angles), labels=labels)\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    fig.legend()\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "f = spider_line(train_df)\n",
    "f = spider_line(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal thresholds:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_nr = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for model_index, model_iter in enumerate(loaded_models):\n",
    "        # Set the model to evaluation mode\n",
    "        (model_name, model) = model_iter\n",
    "        model.eval()\n",
    "\n",
    "        predictions_0 = model(X_test_w)\n",
    "        predictions_1 = model(X_test_nw)\n",
    "\n",
    "        # Calculate AUCs for sensitive attribute 0 \n",
    "        fpr_0, tpr_0, thresholds_0 = roc_curve(y_test_w, predictions_0)\n",
    "        auc_0 = auc(1-fpr_0, 1-tpr_0)  # AUC for FNR is calculated using TPR as x-axis and 1-FPR as y-axis\n",
    "        tnr_minus_fnr_0 = (1-fpr_0) - (1-tpr_0)\n",
    "        # Find the threshold that maximizes TPR - FPR difference\n",
    "        optimal_threshold_index_0 = np.argmax(tnr_minus_fnr_0)\n",
    "        optimal_threshold_0 = thresholds_0[optimal_threshold_index_0]\n",
    "\n",
    "        # Calculate AUCs for sensitive attribute 1 \n",
    "        fpr_1, tpr_1, thresholds_1 = roc_curve(y_test_nw, predictions_1)\n",
    "        auc_1 = auc(1-fpr_1, 1-tpr_1)  # AUC for TNR is calculated using 1-FPR as x-axis and TPR as y-axis\n",
    "        tnr_minus_fnr_1 = (1-fpr_1) - (1-tpr_1)\n",
    "        # Find the threshold that maximizes TPR - FPR difference\n",
    "        optimal_threshold_index_1 = np.argmax(tnr_minus_fnr_1)\n",
    "        optimal_threshold_1 = thresholds_1[optimal_threshold_index_1]\n",
    "        \n",
    "        auc_hm = (auc_0*auc_1)/(auc_0+auc_1)\n",
    "        results_list_nr.append({'Model': str(model_name),\n",
    "                                            'AUC_Sensitive_0': auc_0,\n",
    "                                            'AUC_Sensitive_1': auc_1,\n",
    "                                            'Optimal_Threshold_0': optimal_threshold_0,\n",
    "                                            'Optimal_Threshold_1': optimal_threshold_1,\n",
    "                                            'AUC_HM': auc_hm\n",
    "                                            })\n",
    "        \n",
    "    results_df_nr = pd.DataFrame(results_list_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of predictions by group:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_0 = {}\n",
    "predictions_1 = {}\n",
    "\n",
    "for model_name, model in loaded_models:\n",
    "    preds_0 = torch.nn.functional.sigmoid(model(X_test_w)).detach().numpy()\n",
    "    preds_1 = torch.nn.functional.sigmoid(model(X_test_nw)).detach().numpy()\n",
    "    try:\n",
    "        predictions_0[model_name].append(preds_0)\n",
    "        predictions_1[model_name].append(preds_1)\n",
    "    except:\n",
    "        predictions_0[model_name] = [preds_0]\n",
    "        predictions_1[model_name] = [preds_1]\n",
    "\n",
    "for name in np.unique([name for name, _ in loaded_models]):\n",
    "    predictions_0[name] = np.concatenate(predictions_0[name])\n",
    "    predictions_1[name] = np.concatenate(predictions_1[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for model_name in np.unique([name for name, _ in loaded_models]):\n",
    "\n",
    "    # predictions_0 = torch.nn.functional.sigmoid(model(X_test_w)).detach().numpy()\n",
    "    # predictions_1 = torch.nn.functional.sigmoid(model(X_test_nw)).detach().numpy()\n",
    "    \n",
    "    sns.kdeplot(predictions_0[model_name].squeeze(), label=sensitive_value_0, color='blue', fill=True,bw_adjust=.4)#,clip=[0,1],common_norm=True)\n",
    "    sns.kdeplot(predictions_1[model_name].squeeze(), label=sensitive_value_1, color='red', fill=True,bw_adjust=.4)#,clip=[0,1],common_norm=True)\n",
    "    plt.xlim(-0.1, 1.1)\n",
    "    plt.ylim(0,22)\n",
    "    plt.xlabel('Predictions', fontsize=20)\n",
    "    plt.ylabel('Density', fontsize=20)\n",
    "    # plt.title(model_name, fontsize=10)\n",
    "    # plt.title(alg)\n",
    "    # print(alg)\n",
    "    alg_name = 'sslalm_aug' if model_name.startswith('sslalm_mu0') else model_name.split('_')[0]\n",
    "    plt.savefig(f'./plots/{alg_name}/{DATASET}/dist')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We choose one model per algorithm to make some useful plots**\n",
    "\n",
    "For now, choose the model with the highest mean AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_by = 'auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "algs = res_df_test.Algorithm.unique()\n",
    "for alg in algs:\n",
    "    alg_df = res_df_test[res_df_test.Algorithm == alg]\n",
    "    if select_by == 'auc':\n",
    "        model = loaded_models[alg_df.AUC_M.idxmax()]\n",
    "    elif select_by == 'wd':\n",
    "        model = loaded_models[alg_df.Wd.idxmin()]\n",
    "    best_models[alg] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subgroup ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TPR-FPR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_pr(ax, predictions, targets, sensitive_value):\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(fpr, tpr, label=f'Sensitive={sensitive_value}, AUC = {roc_auc:.2f}')\n",
    "    tpr_minus_fpr = tpr - fpr\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tpr_minus_fpr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(fpr[optimal_threshold_index], tpr[optimal_threshold_index],\n",
    "                c='blue' if sensitive_value == sensitive_value_0 else 'red',\n",
    "                label=f'Optimal Threshold {sensitive_value} {optimal_threshold:.2f}')\n",
    "    \n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    ax  =  f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        predictions_0 = model(X_test_w)\n",
    "        predictions_1 = model(X_test_nw)\n",
    "        # Plot ROC for sensitive attribute A=0\n",
    "        plot_roc_curve_pr(ax, predictions_0, y_test_w, sensitive_value=sensitive_value_0)\n",
    "        # Plot ROC for sensitive attribute A=1\n",
    "        plot_roc_curve_pr(ax, predictions_1, y_test_nw, sensitive_value=sensitive_value_1)\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n",
    "        ax.set_xlabel('False Positive Rate', fontsize=24)\n",
    "        ax.set_ylabel('True Positive Rate', fontsize=24)\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TNR-FNR plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Function to generate predictions and plot ROC curve\n",
    "def plot_roc_curve_nr(ax, predictions, targets, sensitive_value):\n",
    "    # Convert PyTorch tensors to numpy arrays\n",
    "    #predictions = predictions.detach().numpy()\n",
    "    #targets = targets.numpy()\n",
    "\n",
    "    # Compute ROC curve and area under the curve\n",
    "    fpr, tpr, thresholds = roc_curve(targets, predictions)\n",
    "    fnr = (1-tpr)\n",
    "    tnr = (1-fpr)\n",
    "    roc_auc = auc(tnr, fnr)\n",
    "    # Plot ROC curve\n",
    "    ax.plot(tnr, fnr, label=f'Sensitive={sensitive_value}, AUC = {roc_auc:.2f}')\n",
    "\n",
    "    tnr_minus_fnr = tnr - fnr\n",
    "\n",
    "    # Find the threshold that maximizes TPR - FPR difference\n",
    "    optimal_threshold_index = np.argmax(tnr_minus_fnr)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    ax.scatter(tnr[optimal_threshold_index],fnr[optimal_threshold_index],\n",
    "                c='blue' if sensitive_value == sensitive_value_0 else 'red',\n",
    "                label=f'Optimal Threshold {sensitive_value} {optimal_threshold:.2f}')\n",
    "    \n",
    "for alg, (model_name, model) in best_models.items():\n",
    "    f = plt.figure()\n",
    "    ax  =  f.subplots()\n",
    "    ax.set_title(alg)\n",
    "    with torch.inference_mode():\n",
    "        predictions_0 = model(X_test_w)\n",
    "        predictions_1 = model(X_test_nw)\n",
    "        # Plot ROC for sensitive attribute A=0\n",
    "        plot_roc_curve_nr(ax, predictions_0, y_test_w, sensitive_value=sensitive_value_0)\n",
    "        # Plot ROC for sensitive attribute A=1\n",
    "        plot_roc_curve_nr(ax, predictions_1, y_test_nw, sensitive_value=sensitive_value_1)\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier')\n",
    "        ax.set_xlabel('False Negative Rate', fontsize=24)\n",
    "        ax.set_ylabel('True Negative Rate', fontsize=24)\n",
    "        ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcomp-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
